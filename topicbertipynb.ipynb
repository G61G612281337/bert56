{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topicbertipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyQl5HrcwrMO"
      },
      "outputs": [],
      "source": [
        "# Author: Dimo Angelov\n",
        "#\n",
        "# License: BSD 3 clause\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import strip_tags\n",
        "import umap\n",
        "import hdbscan\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from joblib import dump, load\n",
        "from sklearn.cluster import dbscan\n",
        "import tempfile\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy.special import softmax\n",
        "\n",
        "try:\n",
        "    import hnswlib\n",
        "\n",
        "    _HAVE_HNSWLIB = True\n",
        "except ImportError:\n",
        "    _HAVE_HNSWLIB = False\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_hub as hub\n",
        "    import tensorflow_text\n",
        "\n",
        "    _HAVE_TENSORFLOW = True\n",
        "except ImportError:\n",
        "    _HAVE_TENSORFLOW = False\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    _HAVE_TORCH = True\n",
        "except ImportError:\n",
        "    _HAVE_TORCH = False\n",
        "\n",
        "logger = logging.getLogger('top2vec')\n",
        "logger.setLevel(logging.WARNING)\n",
        "sh = logging.StreamHandler()\n",
        "sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
        "logger.addHandler(sh)\n",
        "\n",
        "use_models = [\"universal-sentence-encoder-multilingual\",\n",
        "              \"universal-sentence-encoder\",\n",
        "              \"universal-sentence-encoder-large\",\n",
        "              \"universal-sentence-encoder-multilingual-large\"]\n",
        "\n",
        "use_model_urls = {\n",
        "    \"universal-sentence-encoder-multilingual\": \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\",\n",
        "    \"universal-sentence-encoder\": \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "    \"universal-sentence-encoder-large\": \"https://tfhub.dev/google/universal-sentence-encoder-large/5\",\n",
        "    \"universal-sentence-encoder-multilingual-large\": \"https://tfhub.dev/google/universal-sentence-encoder\"\n",
        "                                                     \"-multilingual-large/3\"\n",
        "}\n",
        "\n",
        "sbert_models = [\"distiluse-base-multilingual-cased\",\n",
        "                \"all-MiniLM-L6-v2\",\n",
        "                \"paraphrase-multilingual-MiniLM-L12-v2\"]\n",
        "\n",
        "acceptable_embedding_models = use_models + sbert_models\n",
        "\n",
        "\n",
        "def default_tokenizer(document):\n",
        "\n",
        "    return simple_preprocess(strip_tags(document), deacc=True)\n",
        "\n",
        "\n",
        "def get_chunks(tokens, chunk_length, max_num_chunks, chunk_overlap_ratio):\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "    if num_tokens == 0:\n",
        "        return [\"\"]\n",
        "\n",
        "    num_chunks = int(np.ceil(num_tokens / chunk_length))\n",
        "\n",
        "    if max_num_chunks is not None:\n",
        "        num_chunks = min(num_chunks, max_num_chunks)\n",
        "\n",
        "    return [\" \".join(tokens[i:i + chunk_length])\n",
        "            for i in list(range(0, num_tokens, int(chunk_length * (1 - chunk_overlap_ratio))))[0:num_chunks]]\n",
        "\n",
        "\n",
        "def get_random_chunks(tokens, chunk_length, chunk_len_coverage_ratio, max_num_chunks):\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "    if num_tokens == 0:\n",
        "        return [\"\"]\n",
        "\n",
        "    num_chunks = int(np.ceil(num_tokens * chunk_len_coverage_ratio / chunk_length))\n",
        "\n",
        "    if max_num_chunks is not None:\n",
        "        num_chunks = min(num_chunks, max_num_chunks)\n",
        "\n",
        "    starts = np.random.choice(range(0, num_tokens), size=num_chunks)\n",
        "    return [\" \".join(tokens[i:i + chunk_length]) for i in starts]\n",
        "\n",
        "\n",
        "class Top2Vec:\n",
        "    \n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 documents,\n",
        "                 min_count=50,\n",
        "                 embedding_model='doc2vec',\n",
        "                 embedding_model_path=None,\n",
        "                 embedding_batch_size=32,\n",
        "                 split_documents=False,\n",
        "                 document_chunker='sequential',\n",
        "                 chunk_length=100,\n",
        "                 max_num_chunks=None,\n",
        "                 chunk_overlap_ratio=0.5,\n",
        "                 chunk_len_coverage_ratio=1.0,\n",
        "                 sentencizer=None,\n",
        "                 speed='learn',\n",
        "                 use_corpus_file=False,\n",
        "                 document_ids=None,\n",
        "                 keep_documents=True,\n",
        "                 workers=None,\n",
        "                 tokenizer=None,\n",
        "                 use_embedding_model_tokenizer=False,\n",
        "                 umap_args=None,\n",
        "                 hdbscan_args=None,\n",
        "                 verbose=True\n",
        "                 ):\n",
        "\n",
        "        if verbose:\n",
        "            logger.setLevel(logging.DEBUG)\n",
        "            self.verbose = True\n",
        "        else:\n",
        "            logger.setLevel(logging.WARNING)\n",
        "            self.verbose = False\n",
        "\n",
        "        if tokenizer is None:\n",
        "            tokenizer = default_tokenizer\n",
        "\n",
        "        # validate documents\n",
        "        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n",
        "            raise ValueError(\"Documents need to be a list of strings\")\n",
        "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
        "            raise ValueError(\"Documents need to be a list of strings\")\n",
        "        if keep_documents:\n",
        "            self.documents = np.array(documents, dtype=\"object\")\n",
        "        else:\n",
        "            self.documents = None\n",
        "\n",
        "        # validate document ids\n",
        "        if document_ids is not None:\n",
        "            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n",
        "                raise ValueError(\"Documents ids need to be a list of str or int\")\n",
        "\n",
        "            if len(documents) != len(document_ids):\n",
        "                raise ValueError(\"Document ids need to match number of documents\")\n",
        "            elif len(document_ids) != len(set(document_ids)):\n",
        "                raise ValueError(\"Document ids need to be unique\")\n",
        "\n",
        "            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
        "                self.doc_id_type = np.str_\n",
        "            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
        "                self.doc_id_type = np.int_\n",
        "            else:\n",
        "                raise ValueError(\"Document ids need to be str or int\")\n",
        "\n",
        "            self.document_ids_provided = True\n",
        "            self.document_ids = np.array(document_ids)\n",
        "            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n",
        "        else:\n",
        "            self.document_ids_provided = False\n",
        "            self.document_ids = np.array(range(0, len(documents)))\n",
        "            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n",
        "            self.doc_id_type = np.int_\n",
        "\n",
        "        self.embedding_model_path = embedding_model_path\n",
        "\n",
        "        # validate document splitting\n",
        "        use_sentencizer = False\n",
        "        custom_chunker = False\n",
        "        if split_documents:\n",
        "            if document_chunker == 'sequential':\n",
        "                document_chunker = get_chunks\n",
        "                document_chunker_args = {\"chunk_length\": chunk_length,\n",
        "                                         \"max_num_chunks\": max_num_chunks,\n",
        "                                         \"chunk_overlap_ratio\": chunk_overlap_ratio}\n",
        "\n",
        "            elif document_chunker == 'random':\n",
        "                document_chunker = get_random_chunks\n",
        "                document_chunker_args = {\"chunk_length\": chunk_length,\n",
        "                                         \"max_num_chunks\": max_num_chunks,\n",
        "                                         \"chunk_len_coverage_ratio\": chunk_len_coverage_ratio}\n",
        "\n",
        "            elif callable(document_chunker):\n",
        "                custom_chunker = True\n",
        "            elif sentencizer is None:\n",
        "                raise ValueError(f\"{document_chunker} is an invalid document chunker.\")\n",
        "            elif callable(sentencizer):\n",
        "                use_sentencizer = True\n",
        "            else:\n",
        "                raise ValueError(f\"{sentencizer} is invalid. Document sentencizer must be callable.\")\n",
        "\n",
        "        if embedding_model == 'doc2vec':\n",
        "\n",
        "            # validate training inputs\n",
        "            if speed == \"fast-learn\":\n",
        "                hs = 0\n",
        "                negative = 5\n",
        "                epochs = 40\n",
        "            elif speed == \"learn\":\n",
        "                hs = 1\n",
        "                negative = 0\n",
        "                epochs = 40\n",
        "            elif speed == \"deep-learn\":\n",
        "                hs = 1\n",
        "                negative = 0\n",
        "                epochs = 400\n",
        "            elif speed == \"test-learn\":\n",
        "                hs = 0\n",
        "                negative = 5\n",
        "                epochs = 1\n",
        "            else:\n",
        "                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n",
        "\n",
        "            if workers is None:\n",
        "                pass\n",
        "            elif isinstance(workers, int):\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(\"workers needs to be an int\")\n",
        "\n",
        "            doc2vec_args = {\"vector_size\": 300,\n",
        "                            \"min_count\": min_count,\n",
        "                            \"window\": 15,\n",
        "                            \"sample\": 1e-5,\n",
        "                            \"negative\": negative,\n",
        "                            \"hs\": hs,\n",
        "                            \"epochs\": epochs,\n",
        "                            \"dm\": 0,\n",
        "                            \"dbow_words\": 1}\n",
        "\n",
        "            if workers is not None:\n",
        "                doc2vec_args[\"workers\"] = workers\n",
        "\n",
        "            logger.info('Pre-processing documents for training')\n",
        "\n",
        "            if use_corpus_file:\n",
        "                processed = [' '.join(tokenizer(doc)) for doc in documents]\n",
        "                lines = \"\\n\".join(processed)\n",
        "                temp = tempfile.NamedTemporaryFile(mode='w+t')\n",
        "                temp.write(lines)\n",
        "                doc2vec_args[\"corpus_file\"] = temp.name\n",
        "\n",
        "            else:\n",
        "                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n",
        "                doc2vec_args[\"documents\"] = train_corpus\n",
        "\n",
        "            logger.info('Creating joint document/word embedding')\n",
        "            self.embedding_model = 'doc2vec'\n",
        "            self.model = Doc2Vec(**doc2vec_args)\n",
        "\n",
        "            self.word_vectors = self.model.wv.get_normed_vectors()\n",
        "            self.word_indexes = self.model.wv.key_to_index\n",
        "            self.vocab = list(self.model.wv.key_to_index.keys())\n",
        "            self.document_vectors = self.model.dv.get_normed_vectors()\n",
        "\n",
        "            if use_corpus_file:\n",
        "                temp.close()\n",
        "\n",
        "        elif (embedding_model in acceptable_embedding_models) or callable(embedding_model):\n",
        "\n",
        "            self.embed = None\n",
        "            self.embedding_model = embedding_model\n",
        "\n",
        "            self._check_import_status()\n",
        "\n",
        "            logger.info('Pre-processing documents for training')\n",
        "\n",
        "            # preprocess documents\n",
        "            tokenized_corpus = [tokenizer(doc) for doc in documents]\n",
        "\n",
        "            def return_doc(doc):\n",
        "                return doc\n",
        "\n",
        "            # preprocess vocabulary\n",
        "            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n",
        "            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n",
        "            words = vectorizer.get_feature_names()\n",
        "            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n",
        "            vocab_inds = np.where(word_counts > min_count)[0]\n",
        "\n",
        "            if len(vocab_inds) == 0:\n",
        "                raise ValueError(f\"A min_count of {min_count} results in \"\n",
        "                                 f\"all words being ignored, choose a lower value.\")\n",
        "            self.vocab = [words[ind] for ind in vocab_inds]\n",
        "\n",
        "            self._check_model_status()\n",
        "\n",
        "            logger.info('Creating joint document/word embedding')\n",
        "\n",
        "            # embed words\n",
        "            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n",
        "            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n",
        "\n",
        "            # embed documents\n",
        "\n",
        "            # split documents\n",
        "            if split_documents:\n",
        "                if use_sentencizer:\n",
        "                    chunk_id = 0\n",
        "                    chunked_docs = []\n",
        "                    chunked_doc_ids = []\n",
        "                    for doc in documents:\n",
        "                        doc_chunks = sentencizer(doc)\n",
        "                        doc_chunk_ids = [chunk_id] * len(doc_chunks)\n",
        "                        chunk_id += 1\n",
        "                        chunked_docs.extend(doc_chunks)\n",
        "                        chunked_doc_ids.extend(doc_chunk_ids)\n",
        "\n",
        "                else:\n",
        "                    chunk_id = 0\n",
        "                    chunked_docs = []\n",
        "                    chunked_doc_ids = []\n",
        "                    for tokens in tokenized_corpus:\n",
        "                        if custom_chunker:\n",
        "                            doc_chunks = document_chunker(tokens)\n",
        "                        else:\n",
        "                            doc_chunks = document_chunker(tokens, **document_chunker_args)\n",
        "                        doc_chunk_ids = [chunk_id] * len(doc_chunks)\n",
        "                        chunk_id += 1\n",
        "                        chunked_docs.extend(doc_chunks)\n",
        "                        chunked_doc_ids.extend(doc_chunk_ids)\n",
        "\n",
        "                chunked_doc_ids = np.array(chunked_doc_ids)\n",
        "                document_chunk_vectors = self._embed_documents(chunked_docs, embedding_batch_size)\n",
        "                self.document_vectors = self._l2_normalize(\n",
        "                    np.vstack([document_chunk_vectors[np.where(chunked_doc_ids == label)[0]]\n",
        "                              .mean(axis=0) for label in set(chunked_doc_ids)]))\n",
        "\n",
        "            # original documents\n",
        "            else:\n",
        "                if use_embedding_model_tokenizer:\n",
        "                    self.document_vectors = self._embed_documents(documents, embedding_batch_size)\n",
        "                else:\n",
        "                    train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
        "                    self.document_vectors = self._embed_documents(train_corpus, embedding_batch_size)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n",
        "\n",
        "        # create 5D embeddings of documents\n",
        "        logger.info('Creating lower dimension embedding of documents')\n",
        "\n",
        "        if umap_args is None:\n",
        "            umap_args = {'n_neighbors': 15,\n",
        "                         'n_components': 5,\n",
        "                         'metric': 'cosine'}\n",
        "\n",
        "        umap_model = umap.UMAP(**umap_args).fit(self.document_vectors)\n",
        "\n",
        "        # find dense areas of document vectors\n",
        "        logger.info('Finding dense areas of documents')\n",
        "\n",
        "        if hdbscan_args is None:\n",
        "            hdbscan_args = {'min_cluster_size': 15,\n",
        "                            'metric': 'euclidean',\n",
        "                            'cluster_selection_method': 'eom'}\n",
        "\n",
        "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
        "\n",
        "        # calculate topic vectors from dense areas of documents\n",
        "        logger.info('Finding topics')\n",
        "\n",
        "        # create topic vectors\n",
        "        self._create_topic_vectors(cluster.labels_)\n",
        "\n",
        "        # deduplicate topics\n",
        "        self._deduplicate_topics()\n",
        "\n",
        "        # find topic words and scores\n",
        "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
        "\n",
        "        # assign documents to topic\n",
        "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
        "                                                                      self.document_vectors)\n",
        "\n",
        "        # calculate topic sizes\n",
        "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
        "\n",
        "        # re-order topics\n",
        "        self._reorder_topics(hierarchy=False)\n",
        "\n",
        "        # initialize variables for hierarchical topic reduction\n",
        "        self.topic_vectors_reduced = None\n",
        "        self.doc_top_reduced = None\n",
        "        self.doc_dist_reduced = None\n",
        "        self.topic_sizes_reduced = None\n",
        "        self.topic_words_reduced = None\n",
        "        self.topic_word_scores_reduced = None\n",
        "        self.hierarchy = None\n",
        "\n",
        "        # initialize document indexing variables\n",
        "        self.document_index = None\n",
        "        self.serialized_document_index = None\n",
        "        self.documents_indexed = False\n",
        "        self.index_id2doc_id = None\n",
        "        self.doc_id2index_id = None\n",
        "\n",
        "        # initialize word indexing variables\n",
        "        self.word_index = None\n",
        "        self.serialized_word_index = None\n",
        "        self.words_indexed = False\n",
        "\n",
        "    def save(self, file):\n",
        "\n",
        "\n",
        "        document_index_temp = None\n",
        "        word_index_temp = None\n",
        "\n",
        "        # do not save sentence encoders, sentence transformers and custom embedding\n",
        "        if self.embedding_model not in [\"doc2vec\"]:\n",
        "            self.embed = None\n",
        "\n",
        "        # serialize document index so that it can be saved\n",
        "        if self.documents_indexed:\n",
        "            temp = tempfile.NamedTemporaryFile(mode='w+b')\n",
        "            self.document_index.save_index(temp.name)\n",
        "            self.serialized_document_index = temp.read()\n",
        "            temp.close()\n",
        "            document_index_temp = self.document_index\n",
        "            self.document_index = None\n",
        "\n",
        "        # serialize word index so that it can be saved\n",
        "        if self.words_indexed:\n",
        "            temp = tempfile.NamedTemporaryFile(mode='w+b')\n",
        "            self.word_index.save_index(temp.name)\n",
        "            self.serialized_word_index = temp.read()\n",
        "            temp.close()\n",
        "            word_index_temp = self.word_index\n",
        "            self.word_index = None\n",
        "\n",
        "        dump(self, file)\n",
        "\n",
        "        self.document_index = document_index_temp\n",
        "        self.word_index = word_index_temp\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "\n",
        "        top2vec_model = load(file)\n",
        "\n",
        "        # load document index\n",
        "        if top2vec_model.documents_indexed:\n",
        "            if not _HAVE_HNSWLIB:\n",
        "                raise ImportError(f\"Cannot load document index.\\n\\n\"\n",
        "                                  \"Try: pip install top2vec[indexing]\\n\\n\"\n",
        "                                  \"Alternatively try: pip install hnswlib\")\n",
        "\n",
        "            temp = tempfile.NamedTemporaryFile(mode='w+b')\n",
        "            temp.write(top2vec_model.serialized_document_index)\n",
        "            document_vectors = top2vec_model.document_vectors\n",
        "            top2vec_model.document_index = hnswlib.Index(space='ip',\n",
        "                                                         dim=document_vectors.shape[1])\n",
        "            top2vec_model.document_index.load_index(temp.name, max_elements=document_vectors.shape[0])\n",
        "            temp.close()\n",
        "            top2vec_model.serialized_document_index = None\n",
        "\n",
        "        # load word index\n",
        "        if top2vec_model.words_indexed:\n",
        "\n",
        "            if not _HAVE_HNSWLIB:\n",
        "                raise ImportError(f\"Cannot load word index.\\n\\n\"\n",
        "                                  \"Try: pip install top2vec[indexing]\\n\\n\"\n",
        "                                  \"Alternatively try: pip install hnswlib\")\n",
        "\n",
        "            temp = tempfile.NamedTemporaryFile(mode='w+b')\n",
        "            temp.write(top2vec_model.serialized_word_index)\n",
        "            word_vectors = top2vec_model.word_vectors\n",
        "            top2vec_model.word_index = hnswlib.Index(space='ip',\n",
        "                                                     dim=word_vectors.shape[1])\n",
        "            top2vec_model.word_index.load_index(temp.name, max_elements=word_vectors.shape[0])\n",
        "            temp.close()\n",
        "            top2vec_model.serialized_word_index = None\n",
        "\n",
        "        return top2vec_model\n",
        "\n",
        "    @staticmethod\n",
        "    def _l2_normalize(vectors):\n",
        "\n",
        "        if vectors.ndim == 2:\n",
        "            return normalize(vectors)\n",
        "        else:\n",
        "            return normalize(vectors.reshape(1, -1))[0]\n",
        "\n",
        "    def _embed_documents(self, train_corpus, batch_size):\n",
        "\n",
        "        self._check_import_status()\n",
        "        self._check_model_status()\n",
        "\n",
        "        # embed documents\n",
        "        document_vectors = []\n",
        "\n",
        "        if (self.embedding_model in use_models) or self.embedding_model == \"custom\":\n",
        "\n",
        "            current = 0\n",
        "            batches = int(len(train_corpus) / batch_size)\n",
        "            extra = len(train_corpus) % batch_size\n",
        "\n",
        "            for ind in range(0, batches):\n",
        "                document_vectors.append(self.embed(train_corpus[current:current + batch_size]))\n",
        "                current += batch_size\n",
        "\n",
        "            if extra > 0:\n",
        "                document_vectors.append(self.embed(train_corpus[current:current + extra]))\n",
        "\n",
        "            document_vectors = self._l2_normalize(np.array(np.vstack(document_vectors)))\n",
        "\n",
        "        else:\n",
        "            document_vectors = self.embed(train_corpus, batch_size=batch_size)\n",
        "\n",
        "        return document_vectors\n",
        "\n",
        "    def _embed_query(self, query):\n",
        "        self._check_import_status()\n",
        "        self._check_model_status()\n",
        "\n",
        "        return self._l2_normalize(np.array(self.embed([query])[0]))\n",
        "\n",
        "    def _create_topic_vectors(self, cluster_labels):\n",
        "        unique_labels = set(cluster_labels)\n",
        "        if -1 in unique_labels:\n",
        "            unique_labels.remove(-1)\n",
        "        self.topic_vectors = self._l2_normalize(\n",
        "            np.vstack([self.document_vectors[np.where(cluster_labels == label)[0]]\n",
        "                      .mean(axis=0) for label in unique_labels]))\n",
        "\n",
        "    def _deduplicate_topics(self):\n",
        "        core_samples, labels = dbscan(X=self.topic_vectors,\n",
        "                                      eps=0.1,\n",
        "                                      min_samples=2,\n",
        "                                      metric=\"cosine\")\n",
        "\n",
        "        duplicate_clusters = set(labels)\n",
        "\n",
        "        if len(duplicate_clusters) > 1 or -1 not in duplicate_clusters:\n",
        "\n",
        "            # unique topics\n",
        "            unique_topics = self.topic_vectors[np.where(labels == -1)[0]]\n",
        "\n",
        "            if -1 in duplicate_clusters:\n",
        "                duplicate_clusters.remove(-1)\n",
        "\n",
        "            # merge duplicate topics\n",
        "            for unique_label in duplicate_clusters:\n",
        "                unique_topics = np.vstack(\n",
        "                    [unique_topics, self._l2_normalize(self.topic_vectors[np.where(labels == unique_label)[0]]\n",
        "                                                       .mean(axis=0))])\n",
        "\n",
        "            self.topic_vectors = unique_topics\n",
        "\n",
        "    def _calculate_topic_sizes(self, hierarchy=False):\n",
        "        if hierarchy:\n",
        "            topic_sizes = pd.Series(self.doc_top_reduced).value_counts()\n",
        "        else:\n",
        "            topic_sizes = pd.Series(self.doc_top).value_counts()\n",
        "\n",
        "        return topic_sizes\n",
        "\n",
        "    def _reorder_topics(self, hierarchy=False):\n",
        "\n",
        "        if hierarchy:\n",
        "            self.topic_vectors_reduced = self.topic_vectors_reduced[self.topic_sizes_reduced.index]\n",
        "            self.topic_words_reduced = self.topic_words_reduced[self.topic_sizes_reduced.index]\n",
        "            self.topic_word_scores_reduced = self.topic_word_scores_reduced[self.topic_sizes_reduced.index]\n",
        "            old2new = dict(zip(self.topic_sizes_reduced.index, range(self.topic_sizes_reduced.index.shape[0])))\n",
        "            self.doc_top_reduced = np.array([old2new[i] for i in self.doc_top_reduced])\n",
        "            self.hierarchy = [self.hierarchy[i] for i in self.topic_sizes_reduced.index]\n",
        "            self.topic_sizes_reduced.reset_index(drop=True, inplace=True)\n",
        "        else:\n",
        "            self.topic_vectors = self.topic_vectors[self.topic_sizes.index]\n",
        "            self.topic_words = self.topic_words[self.topic_sizes.index]\n",
        "            self.topic_word_scores = self.topic_word_scores[self.topic_sizes.index]\n",
        "            old2new = dict(zip(self.topic_sizes.index, range(self.topic_sizes.index.shape[0])))\n",
        "            self.doc_top = np.array([old2new[i] for i in self.doc_top])\n",
        "            self.topic_sizes.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_documents_topic(topic_vectors, document_vectors, dist=True, num_topics=None):\n",
        "        batch_size = 10000\n",
        "        doc_top = []\n",
        "        if dist:\n",
        "            doc_dist = []\n",
        "\n",
        "        if document_vectors.shape[0] > batch_size:\n",
        "            current = 0\n",
        "            batches = int(document_vectors.shape[0] / batch_size)\n",
        "            extra = document_vectors.shape[0] % batch_size\n",
        "\n",
        "            for ind in range(0, batches):\n",
        "                res = np.inner(document_vectors[current:current + batch_size], topic_vectors)\n",
        "\n",
        "                if num_topics is None:\n",
        "                    doc_top.extend(np.argmax(res, axis=1))\n",
        "                    if dist:\n",
        "                        doc_dist.extend(np.max(res, axis=1))\n",
        "                else:\n",
        "                    doc_top.extend(np.flip(np.argsort(res), axis=1)[:, :num_topics])\n",
        "                    if dist:\n",
        "                        doc_dist.extend(np.flip(np.sort(res), axis=1)[:, :num_topics])\n",
        "\n",
        "                current += batch_size\n",
        "\n",
        "            if extra > 0:\n",
        "                res = np.inner(document_vectors[current:current + extra], topic_vectors)\n",
        "\n",
        "                if num_topics is None:\n",
        "                    doc_top.extend(np.argmax(res, axis=1))\n",
        "                    if dist:\n",
        "                        doc_dist.extend(np.max(res, axis=1))\n",
        "                else:\n",
        "                    doc_top.extend(np.flip(np.argsort(res), axis=1)[:, :num_topics])\n",
        "                    if dist:\n",
        "                        doc_dist.extend(np.flip(np.sort(res), axis=1)[:, :num_topics])\n",
        "            if dist:\n",
        "                doc_dist = np.array(doc_dist)\n",
        "        else:\n",
        "            res = np.inner(document_vectors, topic_vectors)\n",
        "\n",
        "            if num_topics is None:\n",
        "                doc_top = np.argmax(res, axis=1)\n",
        "                if dist:\n",
        "                    doc_dist = np.max(res, axis=1)\n",
        "            else:\n",
        "                doc_top.extend(np.flip(np.argsort(res), axis=1)[:, :num_topics])\n",
        "                if dist:\n",
        "                    doc_dist.extend(np.flip(np.sort(res), axis=1)[:, :num_topics])\n",
        "\n",
        "        if num_topics is not None:\n",
        "            doc_top = np.array(doc_top)\n",
        "            if dist:\n",
        "                doc_dist = np.array(doc_dist)\n",
        "\n",
        "        if dist:\n",
        "            return doc_top, doc_dist\n",
        "        else:\n",
        "            return doc_top\n",
        "\n",
        "    def _find_topic_words_and_scores(self, topic_vectors):\n",
        "        topic_words = []\n",
        "        topic_word_scores = []\n",
        "\n",
        "        res = np.inner(topic_vectors, self.word_vectors)\n",
        "        top_words = np.flip(np.argsort(res, axis=1), axis=1)\n",
        "        top_scores = np.flip(np.sort(res, axis=1), axis=1)\n",
        "\n",
        "        for words, scores in zip(top_words, top_scores):\n",
        "            topic_words.append([self.vocab[i] for i in words[0:50]])\n",
        "            topic_word_scores.append(scores[0:50])\n",
        "\n",
        "        topic_words = np.array(topic_words)\n",
        "        topic_word_scores = np.array(topic_word_scores)\n",
        "\n",
        "        return topic_words, topic_word_scores\n",
        "\n",
        "    def _assign_documents_to_topic(self, document_vectors, hierarchy=False):\n",
        "\n",
        "        if hierarchy:\n",
        "            doc_top_new, doc_dist_new = self._calculate_documents_topic(self.topic_vectors_reduced,\n",
        "                                                                        document_vectors,\n",
        "                                                                        dist=True)\n",
        "            self.doc_top_reduced = np.append(self.doc_top_reduced, doc_top_new)\n",
        "            self.doc_dist_reduced = np.append(self.doc_dist_reduced, doc_dist_new)\n",
        "\n",
        "            topic_sizes_new = pd.Series(doc_top_new).value_counts()\n",
        "            for top in topic_sizes_new.index.tolist():\n",
        "                self.topic_sizes_reduced[top] += topic_sizes_new[top]\n",
        "            self.topic_sizes_reduced.sort_values(ascending=False, inplace=True)\n",
        "            self._reorder_topics(hierarchy)\n",
        "        else:\n",
        "            doc_top_new, doc_dist_new = self._calculate_documents_topic(self.topic_vectors, document_vectors, dist=True)\n",
        "            self.doc_top = np.append(self.doc_top, doc_top_new)\n",
        "            self.doc_dist = np.append(self.doc_dist, doc_dist_new)\n",
        "\n",
        "            topic_sizes_new = pd.Series(doc_top_new).value_counts()\n",
        "            for top in topic_sizes_new.index.tolist():\n",
        "                self.topic_sizes[top] += topic_sizes_new[top]\n",
        "            self.topic_sizes.sort_values(ascending=False, inplace=True)\n",
        "            self._reorder_topics(hierarchy)\n",
        "\n",
        "    def _unassign_documents_from_topic(self, doc_indexes, hierarchy=False):\n",
        "        if hierarchy:\n",
        "            doc_top_remove = self.doc_top_reduced[doc_indexes]\n",
        "            self.doc_top_reduced = np.delete(self.doc_top_reduced, doc_indexes, 0)\n",
        "            self.doc_dist_reduced = np.delete(self.doc_dist_reduced, doc_indexes, 0)\n",
        "            topic_sizes_remove = pd.Series(doc_top_remove).value_counts()\n",
        "            for top in topic_sizes_remove.index.tolist():\n",
        "                self.topic_sizes_reduced[top] -= topic_sizes_remove[top]\n",
        "            self.topic_sizes_reduced.sort_values(ascending=False, inplace=True)\n",
        "            self._reorder_topics(hierarchy)\n",
        "        else:\n",
        "            doc_top_remove = self.doc_top[doc_indexes]\n",
        "            self.doc_top = np.delete(self.doc_top, doc_indexes, 0)\n",
        "            self.doc_dist = np.delete(self.doc_dist, doc_indexes, 0)\n",
        "            topic_sizes_remove = pd.Series(doc_top_remove).value_counts()\n",
        "            for top in topic_sizes_remove.index.tolist():\n",
        "                self.topic_sizes[top] -= topic_sizes_remove[top]\n",
        "            self.topic_sizes.sort_values(ascending=False, inplace=True)\n",
        "            self._reorder_topics(hierarchy)\n",
        "\n",
        "    def _get_document_ids(self, doc_index):\n",
        "        return self.document_ids[doc_index]\n",
        "\n",
        "    def _get_document_indexes(self, doc_ids):\n",
        "        if self.document_ids is None:\n",
        "            return doc_ids\n",
        "        else:\n",
        "            return [self.doc_id2index[doc_id] for doc_id in doc_ids]\n",
        "\n",
        "    def _words2word_vectors(self, keywords):\n",
        "\n",
        "        return self.word_vectors[[self.word_indexes[word] for word in keywords]]\n",
        "\n",
        "    def _get_combined_vec(self, vecs, vecs_neg):\n",
        "\n",
        "        combined_vector = np.zeros(self.document_vectors.shape[1], dtype=np.float64)\n",
        "        for vec in vecs:\n",
        "            combined_vector += vec\n",
        "        for vec in vecs_neg:\n",
        "            combined_vector -= vec\n",
        "        combined_vector /= (len(vecs) + len(vecs_neg))\n",
        "        combined_vector = self._l2_normalize(combined_vector)\n",
        "\n",
        "        return combined_vector\n",
        "\n",
        "    @staticmethod\n",
        "    def _search_vectors_by_vector(vectors, vector, num_res):\n",
        "        ranks = np.inner(vectors, vector)\n",
        "        indexes = np.flip(np.argsort(ranks)[-num_res:])\n",
        "        scores = np.array([ranks[res] for res in indexes])\n",
        "\n",
        "        return indexes, scores\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_hnswlib_status():\n",
        "        if not _HAVE_HNSWLIB:\n",
        "            raise ImportError(f\"Indexing is not available.\\n\\n\"\n",
        "                              \"Try: pip install top2vec[indexing]\\n\\n\"\n",
        "                              \"Alternatively try: pip install hnswlib\")\n",
        "\n",
        "    def _check_document_index_status(self):\n",
        "        if self.document_index is None:\n",
        "            raise ImportError(\"There is no document index.\\n\\n\"\n",
        "                              \"Call index_document_vectors method before setting use_index=True.\")\n",
        "\n",
        "    def _check_word_index_status(self):\n",
        "        if self.word_index is None:\n",
        "            raise ImportError(\"There is no word index.\\n\\n\"\n",
        "                              \"Call index_word_vectors method before setting use_index=True.\")\n",
        "\n",
        "    def _check_import_status(self):\n",
        "        if self.embedding_model in use_models:\n",
        "            if not _HAVE_TENSORFLOW:\n",
        "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
        "                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n",
        "                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
        "        elif self.embedding_model in sbert_models:\n",
        "            if not _HAVE_TORCH:\n",
        "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
        "                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n",
        "                                  \"Alternatively try: pip install torch sentence_transformers\")\n",
        "\n",
        "    def _check_model_status(self):\n",
        "        if self.embed is None:\n",
        "            if self.verbose is False:\n",
        "                logger.setLevel(logging.DEBUG)\n",
        "\n",
        "            if self.embedding_model in use_models:\n",
        "                if self.embedding_model_path is None:\n",
        "                    logger.info(f'Downloading {self.embedding_model} model')\n",
        "                    module = use_model_urls[self.embedding_model]\n",
        "\n",
        "                else:\n",
        "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
        "                    module = self.embedding_model_path\n",
        "                self.embed = hub.load(module)\n",
        "\n",
        "            elif self.embedding_model in sbert_models:\n",
        "                if self.embedding_model_path is None:\n",
        "                    logger.info(f'Downloading {self.embedding_model} model')\n",
        "                    module = self.embedding_model\n",
        "                else:\n",
        "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
        "                    module = self.embedding_model_path\n",
        "                model = SentenceTransformer(module)\n",
        "                self.embed = model.encode\n",
        "\n",
        "            elif callable(self.embedding_model):\n",
        "                self.embed = self.embedding_model\n",
        "                self.embedding_model = \"custom\"\n",
        "\n",
        "            elif self.embedding_model == \"custom\":\n",
        "                raise ValueError(\"Call set_embedding_model method and pass callable\"\n",
        "                                 \" embedding_model used during training.\")\n",
        "\n",
        "        if self.verbose is False:\n",
        "            logger.setLevel(logging.WARNING)\n",
        "\n",
        "    @staticmethod\n",
        "    def _less_than_zero(num, var_name):\n",
        "        if num < 0:\n",
        "            raise ValueError(f\"{var_name} cannot be less than 0.\")\n",
        "\n",
        "    def _validate_hierarchical_reduction(self):\n",
        "        if self.hierarchy is None:\n",
        "            raise ValueError(\"Hierarchical topic reduction has not been performed.\")\n",
        "\n",
        "    def _validate_hierarchical_reduction_num_topics(self, num_topics):\n",
        "        current_num_topics = len(self.topic_vectors)\n",
        "        if num_topics >= current_num_topics:\n",
        "            raise ValueError(f\"Number of topics must be less than {current_num_topics}.\")\n",
        "\n",
        "    def _validate_num_docs(self, num_docs):\n",
        "        self._less_than_zero(num_docs, \"num_docs\")\n",
        "        document_count = len(self.doc_top)\n",
        "        if num_docs > document_count:\n",
        "            raise ValueError(f\"num_docs cannot exceed the number of documents: {document_count}.\")\n",
        "\n",
        "    def _validate_num_topics(self, num_topics, reduced):\n",
        "        self._less_than_zero(num_topics, \"num_topics\")\n",
        "        if reduced:\n",
        "            topic_count = len(self.topic_vectors_reduced)\n",
        "            if num_topics > topic_count:\n",
        "                raise ValueError(f\"num_topics cannot exceed the number of reduced topics: {topic_count}.\")\n",
        "        else:\n",
        "            topic_count = len(self.topic_vectors)\n",
        "            if num_topics > topic_count:\n",
        "                raise ValueError(f\"num_topics cannot exceed the number of topics: {topic_count}.\")\n",
        "\n",
        "    def _validate_topic_num(self, topic_num, reduced):\n",
        "        self._less_than_zero(topic_num, \"topic_num\")\n",
        "\n",
        "        if reduced:\n",
        "            topic_count = len(self.topic_vectors_reduced) - 1\n",
        "            if topic_num > topic_count:\n",
        "                raise ValueError(f\"Invalid topic number: valid reduced topics numbers are 0 to {topic_count}.\")\n",
        "        else:\n",
        "            topic_count = len(self.topic_vectors) - 1\n",
        "            if topic_num > topic_count:\n",
        "                raise ValueError(f\"Invalid topic number: valid original topics numbers are 0 to {topic_count}.\")\n",
        "\n",
        "    def _validate_topic_search(self, topic_num, num_docs, reduced):\n",
        "        self._less_than_zero(num_docs, \"num_docs\")\n",
        "        if reduced:\n",
        "            if num_docs > self.topic_sizes_reduced[topic_num]:\n",
        "                raise ValueError(f\"Invalid number of documents: reduced topic {topic_num}\"\n",
        "                                 f\" only has {self.topic_sizes_reduced[topic_num]} documents.\")\n",
        "        else:\n",
        "            if num_docs > self.topic_sizes[topic_num]:\n",
        "                raise ValueError(f\"Invalid number of documents: original topic {topic_num}\"\n",
        "                                 f\" only has {self.topic_sizes[topic_num]} documents.\")\n",
        "\n",
        "    def _validate_doc_ids(self, doc_ids, doc_ids_neg):\n",
        "\n",
        "        if not (isinstance(doc_ids, list) or isinstance(doc_ids, np.ndarray)):\n",
        "            raise ValueError(\"doc_ids must be a list of string or int.\")\n",
        "        if not (isinstance(doc_ids_neg, list) or isinstance(doc_ids_neg, np.ndarray)):\n",
        "            raise ValueError(\"doc_ids_neg must be a list of string or int.\")\n",
        "\n",
        "        if isinstance(doc_ids, np.ndarray):\n",
        "            doc_ids = list(doc_ids)\n",
        "        if isinstance(doc_ids_neg, np.ndarray):\n",
        "            doc_ids_neg = list(doc_ids_neg)\n",
        "\n",
        "        doc_ids_all = doc_ids + doc_ids_neg\n",
        "\n",
        "        if self.document_ids is not None:\n",
        "            for doc_id in doc_ids_all:\n",
        "                if doc_id not in self.doc_id2index:\n",
        "                    raise ValueError(f\"{doc_id} is not a valid document id.\")\n",
        "        elif min(doc_ids) < 0:\n",
        "            raise ValueError(f\"{min(doc_ids)} is not a valid document id.\")\n",
        "        elif max(doc_ids) > len(self.doc_top) - 1:\n",
        "            raise ValueError(f\"{max(doc_ids)} is not a valid document id.\")\n",
        "\n",
        "    def _validate_keywords(self, keywords, keywords_neg):\n",
        "        if not (isinstance(keywords, list) or isinstance(keywords, np.ndarray)):\n",
        "            raise ValueError(\"keywords must be a list of strings.\")\n",
        "\n",
        "        if not (isinstance(keywords_neg, list) or isinstance(keywords_neg, np.ndarray)):\n",
        "            raise ValueError(\"keywords_neg must be a list of strings.\")\n",
        "\n",
        "        keywords_lower = [keyword.lower() for keyword in keywords]\n",
        "        keywords_neg_lower = [keyword.lower() for keyword in keywords_neg]\n",
        "\n",
        "        vocab = self.vocab\n",
        "        for word in keywords_lower + keywords_neg_lower:\n",
        "            if word not in vocab:\n",
        "                raise ValueError(f\"'{word}' has not been learned by the model so it cannot be searched.\")\n",
        "\n",
        "        return keywords_lower, keywords_neg_lower\n",
        "\n",
        "    def _validate_document_ids_add_doc(self, documents, document_ids):\n",
        "        if document_ids is None:\n",
        "            raise ValueError(\"Document ids need to be provided.\")\n",
        "        if len(documents) != len(document_ids):\n",
        "            raise ValueError(\"Document ids need to match number of documents.\")\n",
        "        if len(document_ids) != len(set(document_ids)):\n",
        "            raise ValueError(\"Document ids need to be unique.\")\n",
        "\n",
        "        if len(set(document_ids).intersection(self.document_ids)) > 0:\n",
        "            raise ValueError(\"Some document ids already exist in model.\")\n",
        "\n",
        "        if self.doc_id_type == np.str_:\n",
        "            if not all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
        "                raise ValueError(\"Document ids need to be of type str.\")\n",
        "\n",
        "        if self.doc_id_type == np.int_:\n",
        "            if not all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
        "                raise ValueError(\"Document ids need to be of type int.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _validate_documents(documents):\n",
        "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
        "            raise ValueError(\"Documents need to be a list of strings.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _validate_query(query):\n",
        "        if not isinstance(query, str) or isinstance(query, np.str_):\n",
        "            raise ValueError(\"Query needs to be a string.\")\n",
        "\n",
        "    def _validate_vector(self, vector):\n",
        "        if not isinstance(vector, np.ndarray):\n",
        "            raise ValueError(\"Vector needs to be a numpy array.\")\n",
        "        vec_size = self.document_vectors.shape[1]\n",
        "        if not vector.shape[0] == vec_size:\n",
        "            raise ValueError(f\"Vector needs to be of {vec_size} dimensions.\")\n",
        "\n",
        "    def index_document_vectors(self, ef_construction=200, M=64):\n",
        "        \"\"\"\n",
        "        Creates an index of the document vectors using hnswlib. This will\n",
        "        lead to faster search times for models with a large number of\n",
        "        documents. \n",
        "\n",
        "        For more information on hnswlib see: https://github.com/nmslib/hnswlib\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ef_construction: int (Optional default 200)\n",
        "            This parameter controls the trade-off between index construction\n",
        "            time and index accuracy. Larger values will lead to greater\n",
        "            accuracy but will take longer to construct.\n",
        "\n",
        "        M: int (Optional default 64)\n",
        "            This parameter controls the trade-off between both index size as\n",
        "            well as construction time and accuracy. Larger values will lead to\n",
        "            greater accuracy but will result in a larger index as well as\n",
        "            longer construction time.\n",
        "\n",
        "            For more information on the parameters see:\n",
        "            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n",
        "        \"\"\"\n",
        "\n",
        "        self._check_hnswlib_status()\n",
        "\n",
        "        document_vectors = self.document_vectors\n",
        "        vec_dim = document_vectors.shape[1]\n",
        "        num_vecs = document_vectors.shape[0]\n",
        "\n",
        "        index_ids = list(range(0, len(self.document_ids)))\n",
        "\n",
        "        self.index_id2doc_id = dict(zip(index_ids, self.document_ids))\n",
        "        self.doc_id2index_id = dict(zip(self.document_ids, index_ids))\n",
        "\n",
        "        self.document_index = hnswlib.Index(space='ip', dim=vec_dim)\n",
        "        self.document_index.init_index(max_elements=num_vecs, ef_construction=ef_construction, M=M)\n",
        "        self.document_index.add_items(document_vectors, index_ids)\n",
        "        self.documents_indexed = True\n",
        "\n",
        "    def index_word_vectors(self, ef_construction=200, M=64):\n",
        "        \"\"\"\n",
        "        Creates an index of the word vectors using hnswlib. This will\n",
        "        lead to faster search times for models with a large number of\n",
        "        words.\n",
        "\n",
        "        For more information on hnswlib see: https://github.com/nmslib/hnswlib\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ef_construction: int (Optional default 200)\n",
        "            This parameter controls the trade-off between index construction\n",
        "            time and index accuracy. Larger values will lead to greater\n",
        "            accuracy but will take longer to construct.\n",
        "\n",
        "        M: int (Optional default 64)\n",
        "            This parameter controls the trade-off between both index size as\n",
        "            well as construction time and accuracy. Larger values will lead to\n",
        "            greater accuracy but will result in a larger index as well as\n",
        "            longer construction time.\n",
        "\n",
        "            For more information on the parameters see:\n",
        "            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n",
        "        \"\"\"\n",
        "        self._check_hnswlib_status()\n",
        "\n",
        "        word_vectors = self.word_vectors\n",
        "        vec_dim = word_vectors.shape[1]\n",
        "        num_vecs = word_vectors.shape[0]\n",
        "\n",
        "        index_ids = list(range(0, num_vecs))\n",
        "\n",
        "        self.word_index = hnswlib.Index(space='ip', dim=vec_dim)\n",
        "        self.word_index.init_index(max_elements=num_vecs, ef_construction=ef_construction, M=M)\n",
        "        self.word_index.add_items(word_vectors, index_ids)\n",
        "        self.words_indexed = True\n",
        "\n",
        "    def set_embedding_model(self, embedding_model):\n",
        "        \"\"\"\n",
        "        Set the embedding model. This is called after loading a saved Top2Vec\n",
        "        model which was trained with a passed callable embedding_model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding_model: callable\n",
        "            This must be the same embedding model used during training.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if not callable(embedding_model):\n",
        "            raise ValueError(\"embedding_model must be callable.\")\n",
        "\n",
        "        self.embed = embedding_model\n",
        "\n",
        "    def update_embedding_model_path(self, embedding_model_path):\n",
        "        \"\"\"\n",
        "        Update the path of the embedding model to be loaded. The model will\n",
        "        no longer be downloaded but loaded from the path location.\n",
        "\n",
        "        Warning: the model at embedding_model_path must match the\n",
        "        embedding_model parameter type.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding_model_path: Str\n",
        "            Path to downloaded embedding model.\n",
        "\n",
        "        \"\"\"\n",
        "        self.embedding_model_path = embedding_model_path\n",
        "\n",
        "    def change_to_download_embedding_model(self):\n",
        "        \"\"\"\n",
        "        Use automatic download to load embedding model used for training.\n",
        "        Top2Vec will no longer try and load the embedding model from a file\n",
        "        if a embedding_model path was previously added.\n",
        "\n",
        "        \"\"\"\n",
        "        self.embedding_model_path = None\n",
        "\n",
        "    def get_documents_topics(self, doc_ids, reduced=False, num_topics=1):\n",
        "        \"\"\"\n",
        "        Get document topics.\n",
        "\n",
        "        The topic of each document will be returned.\n",
        "\n",
        "        The corresponding original topics are returned unless reduced=True,\n",
        "        in which case the reduced topics will be returned.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        doc_ids: List of str, int\n",
        "            A unique value per document that is used for referring to\n",
        "            documents in search results. If ids were not given to the model,\n",
        "            the index of each document in the model is the id.\n",
        "\n",
        "        reduced: bool (Optional, default False)\n",
        "            Original topics are returned by default. If True the\n",
        "            reduced topics will be returned.\n",
        "\n",
        "        num_topics: int (Optional, default 1)\n",
        "            The number of topics to return per document.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        topic_nums: array of int, shape(len(doc_ids), num_topics)\n",
        "            The topic number(s) of the document corresponding to each doc_id.\n",
        "\n",
        "        topic_score: array of float, shape(len(doc_ids), num_topics)\n",
        "            Semantic similarity of document to topic(s). The cosine similarity\n",
        "            of the document and topic vector.\n",
        "\n",
        "        topics_words: array of shape(len(doc_ids), num_topics, 50)\n",
        "            For each topic the top 50 words are returned, in order\n",
        "            of semantic similarity to topic.\n",
        "\n",
        "            Example:\n",
        "            [['data', 'deep', 'learning' ... 'artificial'],          <Topic 4>\n",
        "            ['environment', 'warming', 'climate ... 'temperature']  <Topic 21>\n",
        "            ...]\n",
        "\n",
        "        word_scores: array of shape(num_topics, 50)\n",
        "            For each topic the cosine similarity scores of the\n",
        "            top 50 words to the topic are returned.\n",
        "\n",
        "            Example:\n",
        "            [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 4>\n",
        "            [0.7818', 0.7671, 0.7603 ... 0.6769]  <Topic 21>\n",
        "            ...]\n",
        "\n",
        "        \"\"\"\n",
        "        if reduced:\n",
        "            self._validate_hierarchical_reduction()\n",
        "\n",
        "        # make sure documents exist\n",
        "        self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n",
        "\n",
        "        # get document indexes from ids\n",
        "        doc_indexes = self._get_document_indexes(doc_ids)\n",
        "\n",
        "        if num_topics == 1:\n",
        "            if reduced:\n",
        "                doc_topics = self.doc_top_reduced[doc_indexes]\n",
        "                doc_dist = self.doc_dist_reduced[doc_indexes]\n",
        "                topic_words = self.topic_words_reduced[doc_topics]\n",
        "                topic_word_scores = self.topic_word_scores_reduced[doc_topics]\n",
        "            else:\n",
        "                doc_topics = self.doc_top[doc_indexes]\n",
        "                doc_dist = self.doc_dist[doc_indexes]\n",
        "                topic_words = self.topic_words[doc_topics]\n",
        "                topic_word_scores = self.topic_word_scores[doc_topics]\n",
        "\n",
        "        else:\n",
        "            if reduced:\n",
        "                topic_vectors = self.topic_vectors_reduced\n",
        "            else:\n",
        "                topic_vectors = self.topic_vectors\n",
        "\n",
        "            doc_topics, doc_dist = self._calculate_documents_topic(topic_vectors,\n",
        "                                                                   self.document_vectors[doc_indexes],\n",
        "                                                                   num_topics=num_topics)\n",
        "\n",
        "            topic_words = np.array([self.topic_words[topics] for topics in doc_topics])\n",
        "            topic_word_scores = np.array([self.topic_word_scores[topics] for topics in doc_topics])\n",
        "\n",
        "        return doc_topics, doc_dist, topic_words, topic_word_scores\n",
        "\n",
        "    def add_documents(self,\n",
        "                      documents,\n",
        "                      doc_ids=None,\n",
        "                      tokenizer=None,\n",
        "                      use_embedding_model_tokenizer=False,\n",
        "                      embedding_batch_size=32):\n",
        "        \"\"\"\n",
        "        Update the model with new documents.\n",
        "\n",
        "        The documents will be added to the current model without changing\n",
        "        existing document, word and topic vectors. Topic sizes will be updated.\n",
        "\n",
        "        If adding a large quantity of documents relative to the current model\n",
        "        size, or documents containing a largely new vocabulary, a new model\n",
        "        should be trained for best results.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        documents: List of str\n",
        "\n",
        "        doc_ids: List of str, int (Optional)\n",
        "            Only required when doc_ids were given to the original model.\n",
        "\n",
        "            A unique value per document that will be used for referring to\n",
        "            documents in search results.\n",
        "\n",
        "        tokenizer: callable (Optional, default None)\n",
        "            Override the default tokenization method. If None then\n",
        "            gensim.utils.simple_preprocess will be used.\n",
        "\n",
        "        use_embedding_model_tokenizer: bool (Optional, default False)\n",
        "            If using an embedding model other than doc2vec, use the model's\n",
        "            tokenizer for document embedding.\n",
        "\n",
        "        embedding_batch_size: int (default=32)\n",
        "            Batch size for documents being embedded.\n",
        "        \"\"\"\n",
        "        # if tokenizer is not passed use default\n",
        "        if tokenizer is None:\n",
        "            tokenizer = default_tokenizer\n",
        "\n",
        "        # add documents\n",
        "        self._validate_documents(documents)\n",
        "        if self.documents is not None:\n",
        "            self.documents = np.append(self.documents, documents)\n",
        "\n",
        "        # add document ids\n",
        "        if self.document_ids_provided is True:\n",
        "            self._validate_document_ids_add_doc(documents, doc_ids)\n",
        "            doc_ids_len = len(self.document_ids)\n",
        "            self.document_ids = np.append(self.document_ids, doc_ids)\n",
        "            self.doc_id2index.update(dict(zip(doc_ids, list(range(doc_ids_len, doc_ids_len + len(doc_ids))))))\n",
        "\n",
        "        elif doc_ids is None:\n",
        "            num_docs = len(documents)\n",
        "            start_id = max(self.document_ids) + 1\n",
        "            doc_ids = list(range(start_id, start_id + num_docs))\n",
        "            doc_ids_len = len(self.document_ids)\n",
        "            self.document_ids = np.append(self.document_ids, doc_ids)\n",
        "            self.doc_id2index.update(dict(zip(doc_ids, list(range(doc_ids_len, doc_ids_len + len(doc_ids))))))\n",
        "        else:\n",
        "            raise ValueError(\"doc_ids cannot be used because they were not provided to model during training.\")\n",
        "\n",
        "        if self.embedding_model == \"doc2vec\":\n",
        "            docs_processed = [tokenizer(doc) for doc in documents]\n",
        "            document_vectors = np.vstack([self.model.infer_vector(doc_words=doc,\n",
        "                                                                  alpha=0.025,\n",
        "                                                                  min_alpha=0.01,\n",
        "                                                                  epochs=100) for doc in docs_processed])\n",
        "\n",
        "            document_vectors = self._l2_normalize(document_vectors)\n",
        "            self.document_vectors = np.vstack([self.document_vectors, document_vectors])\n",
        "\n",
        "        else:\n",
        "            if use_embedding_model_tokenizer:\n",
        "                docs_training = documents\n",
        "            else:\n",
        "                docs_processed = [tokenizer(doc) for doc in documents]\n",
        "                docs_training = [' '.join(doc) for doc in docs_processed]\n",
        "            document_vectors = self._embed_documents(docs_training, embedding_batch_size)\n",
        "            self.document_vectors = np.vstack([self.document_vectors, document_vectors])\n",
        "\n",
        "        # update index\n",
        "        if self.documents_indexed:\n",
        "            # update capacity of index\n",
        "            current_max = self.document_index.get_max_elements()\n",
        "            updated_max = current_max + len(documents)\n",
        "            self.document_index.resize_index(updated_max)\n",
        "\n",
        "            # update index_id and doc_ids\n",
        "            start_index_id = max(self.index_id2doc_id.keys()) + 1\n",
        "            new_index_ids = list(range(start_index_id, start_index_id + len(doc_ids)))\n",
        "            self.index_id2doc_id.update(dict(zip(new_index_ids, doc_ids)))\n",
        "            self.doc_id2index_id.update(dict(zip(doc_ids, new_index_ids)))\n",
        "            self.document_index.add_items(document_vectors, new_index_ids)\n",
        "\n",
        "        # update topics\n",
        "        self._assign_documents_to_topic(document_vectors, hierarchy=False)\n",
        "\n",
        "        if self.hierarchy is not None:\n",
        "            self._assign_documents_to_topic(document_vectors, hierarchy=True)\n",
        "\n",
        "    def delete_documents(self, doc_ids):\n",
        "        \"\"\"\n",
        "        Delete documents from current model.\n",
        "\n",
        "        Warning: If document ids were not used in original model, deleting\n",
        "        documents will change the indexes and therefore doc_ids.\n",
        "\n",
        "        The documents will be deleted from the current model without changing\n",
        "        existing document, word and topic vectors. Topic sizes will be updated.\n",
        "\n",
        "        If deleting a large quantity of documents relative to the current model\n",
        "        size a new model should be trained for best results.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        doc_ids: List of str, int\n",
        "\n",
        "            A unique value per document that is used for referring to documents\n",
        "            in search results.\n",
        "        \"\"\"\n",
        "        # make sure documents exist\n",
        "        self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n",
        "\n",
        "        # update index\n",
        "        if self.documents_indexed:\n",
        "            # delete doc_ids from index\n",
        "            index_ids = [self.doc_id2index_id(doc_id) for doc_id in doc_ids]\n",
        "            for index_id in index_ids:\n",
        "                self.document_index.mark_deleted(index_id)\n",
        "            # update index_id and doc_ids\n",
        "            for doc_id in doc_ids:\n",
        "                self.doc_id2index_id.pop(doc_id)\n",
        "            for index_id in index_ids:\n",
        "                self.index_id2doc_id.pop(index_id)\n",
        "\n",
        "        # get document indexes from ids\n",
        "        doc_indexes = self._get_document_indexes(doc_ids)\n",
        "\n",
        "        # delete documents\n",
        "        if self.documents is not None:\n",
        "            self.documents = np.delete(self.documents, doc_indexes, 0)\n",
        "\n",
        "        # delete document ids\n",
        "        if self.document_ids is not None:\n",
        "            for doc_id in doc_ids:\n",
        "                self.doc_id2index.pop(doc_id)\n",
        "            keys = list(self.doc_id2index.keys())\n",
        "            self.document_ids = np.array(keys)\n",
        "            values = list(range(0, len(self.doc_id2index.values())))\n",
        "            self.doc_id2index = dict(zip(keys, values))\n",
        "\n",
        "        # delete document vectors\n",
        "        self.document_vectors = np.delete(self.document_vectors, doc_indexes, 0)\n",
        "\n",
        "        # update topics\n",
        "        self._unassign_documents_from_topic(doc_indexes, hierarchy=False)\n",
        "\n",
        "        if self.hierarchy is not None:\n",
        "            self._unassign_documents_from_topic(doc_indexes, hierarchy=True)\n",
        "\n",
        "    def get_num_topics(self, reduced=False):\n",
        "        \"\"\"\n",
        "        Get number of topics.\n",
        "\n",
        "        This is the number of topics Top2Vec has found in the data by default.\n",
        "        If reduced is True, the number of reduced topics is returned.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        reduced: bool (Optional, default False)\n",
        "            The number of original topics will be returned by default. If True\n",
        "            will return the number of reduced topics, if hierarchical topic\n",
        "            reduction has been performed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        num_topics: int\n",
        "        \"\"\"\n",
        "\n",
        "        if reduced:\n",
        "            self._validate_hierarchical_reduction()\n",
        "            return len(self.topic_vectors_reduced)\n",
        "        else:\n",
        "            return len(self.topic_vectors)\n",
        "\n",
        "    def get_topic_sizes(self, reduced=False):\n",
        "        \"\"\"\n",
        "        Get topic sizes.\n",
        "\n",
        "        The number of documents most similar to each topic. Topics are\n",
        "        in increasing order of size.\n",
        "\n",
        "        The sizes of the original topics is returned unless reduced=True,\n",
        "        in which case the sizes of the reduced topics will be returned.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        reduced: bool (Optional, default False)\n",
        "            Original topic sizes are returned by default. If True the\n",
        "            reduced topic sizes will be returned.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        topic_sizes: array of int, shape(num_topics)\n",
        "            The number of documents most similar to the topic.\n",
        "        topic_nums: array of int, shape(num_topics)\n",
        "            The unique number of every topic will be returned.\n",
        "        \"\"\"\n",
        "        if reduced:\n",
        "            self._validate_hierarchical_reduction()\n",
        "            return np.array(self.topic_sizes_reduced.values), np.array(self.topic_sizes_reduced.index)\n",
        "        else:\n",
        "            return np.array(self.topic_sizes.values), np.array(self.topic_sizes.index)\n",
        "\n",
        "    def get_topics(self, num_topics=None, reduced=False):\n",
        "        \"\"\"\n",
        "        Get topics, ordered by decreasing size. All topics are returned\n",
        "        if num_topics is not specified.\n",
        "\n",
        "        The original topics found are returned unless reduced=True,\n",
        "        in which case reduced topics will be returned.\n",
        "\n",
        "        Each topic will consist of the top 50 semantically similar words\n",
        "        to the topic. These are the 50 words closest to topic vector\n",
        "        along with cosine similarity of each word from vector. The\n",
        "        higher the score the more relevant the word is to the topic.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_topics: int, (Optional)\n",
        "            Number of topics to return.\n",
        "\n",
        "        reduced: bool (Optional, default False)\n",
        "            Original topics are returned by default. If True the\n",
        "            reduced topics will be returned.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        topics_words: array of shape(num_topics, 50)\n",
        "            For each topic the top 50 words are returned, in order\n",
        "            of semantic similarity to topic.\n",
        "            \n",
        "            Example:\n",
        "            [['data', 'deep', 'learning' ... 'artificial'],         <Topic 0>\n",
        "            ['environment', 'warming', 'climate ... 'temperature']  <Topic 1>\n",
        "            ...]\n",
        "\n",
        "        word_scores: array of shape(num_topics, 50)\n",
        "            For each topic the cosine similarity scores of the\n",
        "            top 50 words to the topic are returned.\n",
        "            \n",
        "            Example:\n",
        "            [[0.7132, 0.6473, 0.5700 ... 0.3455],  <Topic 0>\n",
        "            [0.7818', 0.7671, 0.7603 ... 0.6769]   <Topic 1>\n",
        "            ...]\n",
        "\n",
        "        topic_nums: array of int, shape(num_topics)\n",
        "            The unique number of every topic will be returned.\n",
        "        \"\"\"\n",
        "        if reduced:\n",
        "            self._validate_hierarchical_reduction()\n",
        "\n",
        "            if num_topics is None:\n",
        "                num_topics = len(self.topic_vectors_reduced)\n",
        "            else:\n",
        "                self._validate_num_topics(num_topics, reduced)\n",
        "\n",
        "            return self.topic_words_reduced[0:num_topics], self.topic_word_scores_reduced[0:num_topics], np.array(\n",
        "                range(0, num_topics))\n",
        "        else:\n",
        "\n",
        "            if num_topics is None:\n",
        "                num_topics = len(self.topic_vectors)\n",
        "            else:\n",
        "                self._validate_num_topics(num_topics, reduced)\n",
        "\n",
        "            return self.topic_words[0:num_topics], self.topic_word_scores[0:num_topics], np.array(range(0, num_topics))\n",
        "\n",
        "    def get_topic_hierarchy(self):\n",
        "        \"\"\"\n",
        "        Get the hierarchy of reduced topics. The mapping of each original topic\n",
        "        to the reduced topics is returned.\n",
        "\n",
        "        Hierarchical topic reduction must be performed before calling this\n",
        "        method.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        hierarchy: list of ints\n",
        "            Each index of the hierarchy corresponds to the topic number of a\n",
        "            reduced topic. For each reduced topic the topic numbers of the\n",
        "            original topics that were merged to create it are listed.\n",
        "\n",
        "            Example:\n",
        "            [[3]  <Reduced Topic 0> contains original Topic 3\n",
        "            [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n",
        "            [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n",
        "            ...]\n",
        "        \"\"\"\n",
        "\n",
        "        self._validate_hierarchical_reduction()\n",
        "\n",
        "        return self.hierarchy\n",
        "\n",
        "    def hierarchical_topic_reduction(self, num_topics):\n",
        "        \"\"\"\n",
        "        Reduce the number of topics discovered by Top2Vec.\n",
        "\n",
        "        The most representative topics of the corpus will be found, by\n",
        "        iteratively merging each smallest topic to the most similar topic until\n",
        "        num_topics is reached.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_topics: int\n",
        "            The number of topics to reduce to.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        hierarchy: list of ints\n",
        "            Each index of hierarchy corresponds to the reduced topics, for each\n",
        "            reduced topic the indexes of the original topics that were merged\n",
        "            to create it are listed.\n",
        "\n",
        "            Example:\n",
        "            [[3]  <Reduced Topic 0> contains original Topic 3\n",
        "            [2,4] <Reduced Topic 1> contains original Topics 2 and 4\n",
        "            [0,1] <Reduced Topic 3> contains original Topics 0 and 1\n",
        "            ...]\n",
        "        \"\"\"\n",
        "        self._validate_hierarchical_reduction_num_topics(num_topics)\n",
        "\n",
        "        num_topics_current = self.topic_vectors.shape[0]\n",
        "        top_vecs = self.topic_vectors\n",
        "        top_sizes = [self.topic_sizes[i] for i in range(0, len(self.topic_sizes))]\n",
        "        hierarchy = [[i] for i in range(self.topic_vectors.shape[0])]\n",
        "\n",
        "        count = 0\n",
        "        interval = max(int(self.document_vectors.shape[0] / 50000), 1)\n",
        "\n",
        "        while num_topics_current > num_topics:\n",
        "\n",
        "            # find smallest and most similar topics\n",
        "            smallest = np.argmin(top_sizes)\n",
        "            res = np.inner(top_vecs[smallest], top_vecs)\n",
        "            sims = np.flip(np.argsort(res))\n",
        "            most_sim = sims[1]\n",
        "            if most_sim == smallest:\n",
        "                most_sim = sims[0]\n",
        "\n",
        "            # calculate combined topic vector\n",
        "            top_vec_smallest = top_vecs[smallest]\n",
        "            smallest_size = top_sizes[smallest]\n",
        "\n",
        "            top_vec_most_sim = top_vecs[most_sim]\n",
        "            most_sim_size = top_sizes[most_sim]\n",
        "\n",
        "            combined_vec = self._l2_normalize(((top_vec_smallest * smallest_size) +\n",
        "                                               (top_vec_most_sim * most_sim_size)) / (smallest_size + most_sim_size))\n",
        "\n",
        "            # update topic vectors\n",
        "            ix_keep = list(range(len(top_vecs)))\n",
        "            ix_keep.remove(smallest)\n",
        "            ix_keep.remove(most_sim)\n",
        "            top_vecs = top_vecs[ix_keep]\n",
        "            top_vecs = np.vstack([top_vecs, combined_vec])\n",
        "            num_topics_current = top_vecs.shape[0]\n",
        "\n",
        "            # update topics sizes\n",
        "            if count % interval == 0:\n",
        "                doc_top = self._calculate_documents_topic(topic_vectors=top_vecs,\n",
        "                                                          document_vectors=self.document_vectors,\n",
        "                                                          dist=False)\n",
        "                topic_sizes = pd.Series(doc_top).value_counts()\n",
        "                top_sizes = [topic_sizes[i] for i in range(0, len(topic_sizes))]\n",
        "\n",
        "            else:\n",
        "                smallest_size = top_sizes.pop(smallest)\n",
        "                if most_sim < smallest:\n",
        "                    most_sim_size = top_sizes.pop(most_sim)\n",
        "                else:\n",
        "                    most_sim_size = top_sizes.pop(most_sim - 1)\n",
        "                combined_size = smallest_size + most_sim_size\n",
        "                top_sizes.append(combined_size)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            # update topic hierarchy\n",
        "            smallest_inds = hierarchy.pop(smallest)\n",
        "            if most_sim < smallest:\n",
        "                most_sim_inds = hierarchy.pop(most_sim)\n",
        "            else:\n",
        "                most_sim_inds = hierarchy.pop(most_sim - 1)\n",
        "\n",
        "            combined_inds = smallest_inds + most_sim_inds\n",
        "            hierarchy.append(combined_inds)\n",
        "\n",
        "        # re-calculate topic vectors from clusters\n",
        "        doc_top = self._calculate_documents_topic(topic_vectors=top_vecs,\n",
        "                                                  document_vectors=self.document_vectors,\n",
        "                                                  dist=False)\n",
        "        self.topic_vectors_reduced = self._l2_normalize(np.vstack([self.document_vectors\n",
        "                                                                   [np.where(doc_top == label)[0]]\n",
        "                                                                  .mean(axis=0) for label in set(doc_top)]))\n",
        "\n",
        "        self.hierarchy = hierarchy\n",
        "\n",
        "        # assign documents to topic\n",
        "        self.doc_top_reduced, self.doc_dist_reduced = self._calculate_documents_topic(self.topic_vectors_reduced,\n",
        "                                                                                      self.document_vectors)\n",
        "        # find topic words and scores\n",
        "        self.topic_words_reduced, self.topic_word_scores_reduced = self._find_topic_words_and_scores(\n",
        "            topic_vectors=self.topic_vectors_reduced)\n",
        "\n",
        "        # calculate topic sizes\n",
        "        self.topic_sizes_reduced = self._calculate_topic_sizes(hierarchy=True)\n",
        "\n",
        "        # re-order topics\n",
        "        self._reorder_topics(hierarchy=True)\n",
        "\n",
        "        return self.hierarchy\n",
        "\n",
        "    def query_documents(self, query, num_docs, return_documents=True, use_index=False, ef=None, tokenizer=None):\n",
        "        \"\"\"\n",
        "        Semantic search of documents using a text query.\n",
        "\n",
        "        The most semantically similar documents to the query will be returned.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query: string\n",
        "            Any sequence of text. This could be an actual question, a sentence,\n",
        "            a paragraph or a document.\n",
        "\n",
        "        num_docs: int\n",
        "            Number of documents to return.\n",
        "\n",
        "        return_documents: bool (Optional default True)\n",
        "            Determines if the documents will be returned. If they were not\n",
        "            saved in the model they will not be returned.\n",
        "\n",
        "        use_index: bool (Optional default False)\n",
        "            If index_documents method has been called, setting this to True\n",
        "            will speed up search for models with large number of documents.\n",
        "\n",
        "        ef: int (Optional default None)\n",
        "            Higher ef leads to more accurate but slower search. This value\n",
        "            must be higher than num_docs.\n",
        "\n",
        "            For more information see:\n",
        "            https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md\n",
        "\n",
        "        tokenizer: callable (Optional, default None)\n",
        "\n",
        "            ** For doc2vec embedding model only **\n",
        "\n",
        "            Override the default tokenization method. If None then\n",
        "            gensim.utils.simple_preprocess will be used.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        documents: (Optional) array of str, shape(num_docs)\n",
        "            The documents in a list, the most similar are first.\n",
        "\n",
        "            Will only be returned if the documents were saved and if\n",
        "            return_documents is set to True.\n",
        "\n",
        "        doc_scores: array of float, shape(num_docs)\n",
        "            Semantic similarity of document to vector. The cosine similarity of\n",
        "            the document and vector.\n",
        "\n",
        "        doc_ids: array of int, shape(num_docs)\n",
        "            Unique ids of documents. If ids were not given to the model, the\n",
        "            index of the document in the model will be returned.\n",
        "        \"\"\"\n",
        "\n",
        "        self._validate_query(query)\n",
        "        self._validate_num_docs(num_docs)\n",
        "\n",
        "        if self.embedding_model != \"doc2vec\":\n",
        "            query_vec = self._embed_query(query)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # if tokenizer is not passed use default\n",
        "            if tokenizer is None:\n",
        "                tokenizer = default_tokenizer\n",
        "\n",
        "            tokenized_query = tokenizer(query)\n",
        "\n",
        "            query_vec = self.model.infer_vector(doc_words=tokenized_query,\n",
        "                                                alpha=0.025,\n",
        "                                                min_alpha=0.01,\n",
        "                                                epochs=100)\n",
        "\n",
        "        return self.search_documents_by_vector(query_vec, num_docs, return_documents=return_documents,\n",
        "                                               use_index=use_index, ef=ef)\n",
        "\n",
        "    def query_topics(self, query, num_topics, reduced=False, tokenizer=None):\n",
        "        \n",
        "        semantically similar to the vector.\n",
        "\n",
        "\n",
        "\n",
        "        self._validate_query(query)\n",
        "\n",
        "        if self.embedding_model != \"doc2vec\":\n",
        "            query_vec = self._embed_query(query)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # if tokenizer is not passed use default\n",
        "            if tokenizer is None:\n",
        "                tokenizer = default_tokenizer\n",
        "\n",
        "            tokenized_query = tokenizer(query)\n",
        "\n",
        "            query_vec = self.model.infer_vector(doc_words=tokenized_query,\n",
        "                                                alpha=0.025,\n",
        "                                                min_alpha=0.01,\n",
        "                                                epochs=100)\n",
        "\n",
        "        return self.search_topics_by_vector(query_vec, num_topics=num_topics, reduced=reduced)\n",
        "\n",
        "    def search_documents_by_vector(self, vector, num_docs, return_documents=True, use_index=False, ef=None):\n",
        "\n",
        "        self._validate_vector(vector)\n",
        "        self._validate_num_docs(num_docs)\n",
        "\n",
        "        vector = self._l2_normalize(vector)\n",
        "\n",
        "        if use_index:\n",
        "            self._check_document_index_status()\n",
        "\n",
        "            if ef is not None:\n",
        "                self.document_index.set_ef(ef)\n",
        "            else:\n",
        "                self.document_index.set_ef(num_docs)\n",
        "\n",
        "            index_ids, doc_scores = self.document_index.knn_query(vector, k=num_docs)\n",
        "            index_ids = index_ids[0]\n",
        "            doc_ids = np.array([self.index_id2doc_id[index_id] for index_id in index_ids])\n",
        "            doc_scores = doc_scores[0]\n",
        "            doc_scores = np.array([1 - score for score in doc_scores])\n",
        "            doc_indexes = self._get_document_indexes(doc_ids)\n",
        "        else:\n",
        "            doc_indexes, doc_scores = self._search_vectors_by_vector(self.document_vectors,\n",
        "                                                                     vector, num_docs)\n",
        "            doc_ids = self._get_document_ids(doc_indexes)\n",
        "\n",
        "        if self.documents is not None and return_documents:\n",
        "            documents = self.documents[doc_indexes]\n",
        "            return documents, doc_scores, doc_ids\n",
        "        else:\n",
        "            return doc_scores, doc_ids\n",
        "\n",
        "    def search_words_by_vector(self, vector, num_words, use_index=False, ef=None):\n",
        "\n",
        "\n",
        "        self._validate_vector(vector)\n",
        "\n",
        "        vector = self._l2_normalize(vector)\n",
        "\n",
        "        if use_index:\n",
        "            self._check_word_index_status()\n",
        "\n",
        "            if ef is not None:\n",
        "                self.word_index.set_ef(ef)\n",
        "            else:\n",
        "                self.word_index.set_ef(num_words)\n",
        "\n",
        "            word_indexes, word_scores = self.word_index.knn_query(vector, k=num_words)\n",
        "            word_indexes = word_indexes[0]\n",
        "            word_scores = word_scores[0]\n",
        "            word_scores = np.array([1 - score for score in word_scores])\n",
        "\n",
        "        else:\n",
        "            word_indexes, word_scores = self._search_vectors_by_vector(self.word_vectors,\n",
        "                                                                       vector, num_words)\n",
        "\n",
        "        words = np.array([self.vocab[index] for index in word_indexes])\n",
        "\n",
        "        return words, word_scores\n",
        "\n",
        "    def search_topics_by_vector(self, vector, num_topics, reduced=False):\n",
        "        \n",
        "\n",
        "        self._validate_vector(vector)\n",
        "        self._validate_num_topics(num_topics, reduced)\n",
        "\n",
        "        vector = self._l2_normalize(vector)\n",
        "\n",
        "        if reduced:\n",
        "            self._validate_hierarchical_reduction()\n",
        "\n",
        "            topic_nums, topic_scores = self._search_vectors_by_vector(self.topic_vectors_reduced,\n",
        "                                                                      vector, num_topics)\n",
        "            topic_words = [self.topic_words_reduced[topic] for topic in topic_nums]\n",
        "            word_scores = [self.topic_word_scores_reduced[topic] for topic in topic_nums]\n",
        "\n",
        "        else:\n",
        "            topic_nums, topic_scores = self._search_vectors_by_vector(self.topic_vectors,\n",
        "                                                                      vector, num_topics)\n",
        "            topic_words = [self.topic_words[topic] for topic in topic_nums]\n",
        "            word_scores = [self.topic_word_scores[topic] for topic in topic_nums]\n",
        "\n",
        "        return topic_words, word_scores, topic_scores, topic_nums\n",
        "\n",
        "    def search_documents_by_topic(self, topic_num, num_docs, return_documents=True, reduced=False):\n",
        "       \n",
        "       \n",
        "        if reduced:\n",
        "            self._validate_hierarchical_reduction()\n",
        "            self._validate_topic_num(topic_num, reduced)\n",
        "            self._validate_topic_search(topic_num, num_docs, reduced)\n",
        "\n",
        "            topic_document_indexes = np.where(self.doc_top_reduced == topic_num)[0]\n",
        "            topic_document_indexes_ordered = np.flip(np.argsort(self.doc_dist_reduced[topic_document_indexes]))\n",
        "            doc_indexes = topic_document_indexes[topic_document_indexes_ordered][0:num_docs]\n",
        "            doc_scores = self.doc_dist_reduced[doc_indexes]\n",
        "            doc_ids = self._get_document_ids(doc_indexes)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self._validate_topic_num(topic_num, reduced)\n",
        "            self._validate_topic_search(topic_num, num_docs, reduced)\n",
        "\n",
        "            topic_document_indexes = np.where(self.doc_top == topic_num)[0]\n",
        "            topic_document_indexes_ordered = np.flip(np.argsort(self.doc_dist[topic_document_indexes]))\n",
        "            doc_indexes = topic_document_indexes[topic_document_indexes_ordered][0:num_docs]\n",
        "            doc_scores = self.doc_dist[doc_indexes]\n",
        "            doc_ids = self._get_document_ids(doc_indexes)\n",
        "\n",
        "        if self.documents is not None and return_documents:\n",
        "            documents = self.documents[doc_indexes]\n",
        "            return documents, doc_scores, doc_ids\n",
        "        else:\n",
        "            return doc_scores, doc_ids\n"
      ]
    }
  ]
}